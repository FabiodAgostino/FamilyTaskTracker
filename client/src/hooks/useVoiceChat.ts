import { useState, useCallback, useRef } from 'react';
import { useSpeechSynthesis } from '../hooks/useSpeechSynthesis';

interface VoiceChatConfig {
  ttsEndpoint: string;
  onTranscript?: (text: string) => void;
  onTTSStart?: () => void;
  onTTSEnd?: () => void;
  onError?: (error: string) => void;
  onInfo?: (message: string) => void; // âœ… Aggiunto per notifiche fallback
}

export const useVoiceChat = (config: VoiceChatConfig) => {
  const [isVoiceChatActive, setIsVoiceChatActive] = useState(false);
  const isVoiceChatActiveRef = useRef(false);

  const [isListening, setIsListening] = useState(false);
  const [isSpeaking, setIsSpeaking] = useState(false);
  const [isProcessing, setIsProcessing] = useState(false);
  
  const recognitionRef = useRef<SpeechRecognition | null>(null);
  const currentAudioRef = useRef<HTMLAudioElement | null>(null);
  const processingTimeoutRef = useRef<number | null>(null);

  // âœ… USA L'HOOK SPEECH SYNTHESIS ALL'INIZIO
  const { 
    speak: speechSynthesize, 
    cancel: speechCancel, 
    speaking: speechSpeaking,
    supported: speechSupported,
    getItalianVoice
  } = useSpeechSynthesis();

  // âœ… FUNZIONE HELPER PER FALLBACK CON SPEECH SYNTHESIS NATIVO
  const speakWithNativeSynthesis = useCallback(async (text: string, reason: string): Promise<void> => {
    if (!speechSupported) {
      console.error('âŒ Speech Synthesis non supportato in questo browser');
      config.onError?.('Sintesi vocale non supportata dal browser');
      setIsSpeaking(false);
      return;
    }

    console.log(`ðŸ”„ Fallback a Speech Synthesis nativo (${reason}):`, text.substring(0, 50));
    
    try {
      setIsSpeaking(true);
      config.onTTSStart?.();
      
      speechSynthesize({
        text: text.trim(),
        rate: 1.0,
        pitch: 1.0,
        volume: 1.0,
        voice: getItalianVoice(),
        onStart: () => {
          console.log('ðŸ—£ï¸ Speech Synthesis: Avviato');
        },
        onEnd: () => {
          console.log('âœ… Speech Synthesis: Completato');
          setIsSpeaking(false);
          config.onTTSEnd?.();
          
          // ðŸŽ¯ RIATTIVA AUTOMATICAMENTE L'ASCOLTO
          if (isVoiceChatActive) {
            setTimeout(() => {
              startListening();
            }, 500);
          }
        },
        onError: (error) => {
          console.error('âŒ Speech Synthesis Error:', error);
          setIsSpeaking(false);
          config.onError?.('Errore sintesi vocale nativa');
        }
      });
      
    } catch (error) {
      console.error('âŒ Fallback Speech Synthesis failed:', error);
      setIsSpeaking(false);
      config.onError?.('Errore fallback sintesi vocale');
    }
  }, [speechSynthesize, speechSupported, getItalianVoice, config, isVoiceChatActive]);

  // Funzione per iniziare l'ascolto
  const startListening = useCallback(() => {
    if (!('webkitSpeechRecognition' in window) && !('SpeechRecognition' in window)) {
      config.onError?.('Speech Recognition non supportato');
      return;
    }

    // Ferma riconoscimento precedente
    if (recognitionRef.current) {
      recognitionRef.current.stop();
    }

    const SpeechRecognition = window.SpeechRecognition || window.webkitSpeechRecognition;
    const recognition = new SpeechRecognition();

    recognition.continuous = false;
    recognition.interimResults = false;
    recognition.lang = 'it-IT';

    recognition.onstart = () => {
      console.log('ðŸŽ¤ Ascolto iniziato');
      setIsListening(true);
    };

    recognition.onresult = (event) => {
      const transcript = event.results[0][0].transcript;
      console.log('ðŸ“ Trascritto:', transcript);
      
      setIsListening(false);
      setIsProcessing(true);
      
      config.onTranscript?.(transcript);

      // Timeout di sicurezza per il processing
      if (processingTimeoutRef.current) {
        clearTimeout(processingTimeoutRef.current);
      }
      
      processingTimeoutRef.current = window.setTimeout(() => {
        setIsProcessing(false);
        if (isVoiceChatActive) {
          startListening(); // Riprova se non arriva risposta
        }
      }, 10000); // 10 secondi di timeout
    };

    recognition.onerror = (event) => {
      console.error('âŒ Speech Recognition Error:', event.error);
      setIsListening(false);
      
      if (event.error === 'no-speech') {
        // Nessun parlato rilevato - riprova automaticamente
        if (isVoiceChatActive) {
          setTimeout(() => {
            startListening();
          }, 1000);
        }
      } else {
        config.onError?.(`Errore riconoscimento: ${event.error}`);
      }
    };

    recognition.onend = () => {
      console.log('ðŸ Speech Recognition terminato');
      setIsListening(false);
    };

    recognitionRef.current = recognition;
    recognition.start();
  }, [isVoiceChatActive, config]);

  // Converte base64 in blob audio
  const base64ToBlob = useCallback((base64: string, mimeType: string): Blob => {
    const byteCharacters = atob(base64);
    const byteNumbers = new Array(byteCharacters.length);
    
    for (let i = 0; i < byteCharacters.length; i++) {
      byteNumbers[i] = byteCharacters.charCodeAt(i);
    }
    
    const byteArray = new Uint8Array(byteNumbers);
    return new Blob([byteArray], { type: mimeType });
  }, []);

  // âœ… FUNZIONE speakText CORRETTA CON FALLBACK
  const speakText = useCallback(async (text: string): Promise<void> => {
    if (!text.trim()) return;

    setIsSpeaking(true);
    config.onTTSStart?.();

    try {
      console.log('ðŸ”Š TTS: Tentativo con endpoint cloud:', text.substring(0, 50));

      const response = await fetch(config.ttsEndpoint, {
        method: 'POST',
        headers: {
          'Content-Type': 'application/json',
        },
        body: JSON.stringify({ text: text.trim() })
      });
      // âœ… CONTROLLA SE LA RISPOSTA Ãˆ OK
      if (!response.ok) {
        console.log(`âš ï¸ HTTP Error ${response.status}, using native Speech Synthesis`);
        config.onInfo?.(`Servizio cloud non disponibile (${response.status}), uso sintesi nativa`);
        await speakWithNativeSynthesis(text, `HTTP ${response.status}`);
        return;
      }

      const data = await response.json();
      
      // âœ… CONTROLLA SUCCESS E GESTISCI QUOTA EXCEEDED
      if (data.success && data.audioContent) {
        console.log('âœ… TTS Cloud: Sintesi riuscita');
        
        // Ferma audio precedente se presente
        if (currentAudioRef.current) {
          currentAudioRef.current.pause();
          URL.revokeObjectURL(currentAudioRef.current.src);
        }

        // Crea nuovo audio
        const audioBlob = base64ToBlob(data.audioContent, 'audio/mp3');
        const audioUrl = URL.createObjectURL(audioBlob);
        const audio = new Audio(audioUrl);
        
        currentAudioRef.current = audio;

        return new Promise((resolve) => {
          audio.onended = () => {
            setIsSpeaking(false);
            URL.revokeObjectURL(audioUrl);
            config.onTTSEnd?.();
            
            // ðŸŽ¯ RIATTIVA AUTOMATICAMENTE L'ASCOLTO
            if (isVoiceChatActive) {
              setTimeout(() => {
                startListening();
              }, 500);
            }
            
            resolve();
          };
          
          audio.onerror = (err) => {
            console.log('âŒ Audio playback failed, using native Speech Synthesis');
            URL.revokeObjectURL(audioUrl);
            // Fallback se anche la riproduzione audio fallisce
            speakWithNativeSynthesis(text, 'audio playback error');
            resolve(); // Non reject, usiamo fallback
          };
          
          audio.play().catch((playError) => {
            console.log('âŒ Audio play failed, using native Speech Synthesis:', playError);
            URL.revokeObjectURL(audioUrl);
            // Fallback se play() fallisce
            speakWithNativeSynthesis(text, 'audio play error');
            resolve(); // Non reject, usiamo fallback
          });
        });
        
      } else {
        // âœ… SERVIZIO RISPONDE MA CON ERRORE (quota, etc.)
        const errorReason = data.reason || 'service_error';
        const errorMessage = data.error || 'Errore TTS';
        
        console.log(`âš ï¸ TTS Cloud fallito: ${errorMessage}`);
        
        if (errorReason === 'QUOTA_EXCEEDED') {
          console.log('ðŸ“Š Quota mensile raggiunta, uso Speech Synthesis nativo');
          config.onInfo?.('Quota cloud raggiunta, uso sintesi nativa del browser');
        } else {
          console.log('ðŸ”„ Servizio TTS non disponibile, uso Speech Synthesis nativo');
          config.onInfo?.('Servizio cloud non disponibile, uso sintesi nativa');
        }
        
        await speakWithNativeSynthesis(text, errorReason);
      }
      
    } catch (error) {
      // âœ… ERRORE RETE/CONNESSIONE - USA FALLBACK
      console.log('ðŸŒ Errore connessione TTS Cloud, uso Speech Synthesis nativo:', error);
      config.onInfo?.('Connessione non disponibile, uso sintesi nativa del browser');
      await speakWithNativeSynthesis(text, 'network error');
    }
  }, [config, isVoiceChatActive, base64ToBlob, startListening, speakWithNativeSynthesis]);

  // Inizia chat vocale
  const startVoiceChat = useCallback(() => {
    console.log('ðŸš€ Avvio chat vocale');
    setIsVoiceChatActive(true);
    isVoiceChatActiveRef.current = true;
    startListening();
  }, [startListening]);

  // âœ… FERMA CHAT VOCALE MIGLIORATA
  const stopVoiceChat = useCallback(() => {
    console.log('â¹ï¸ Stop chat vocale');
    isVoiceChatActiveRef.current = false;
    setIsVoiceChatActive(false);
    setIsListening(false);
    setIsSpeaking(false);
    setIsProcessing(false);

    // Ferma riconoscimento
    if (recognitionRef.current) {
      recognitionRef.current.stop();
      recognitionRef.current = null;
    }

    // Ferma audio cloud
    if (currentAudioRef.current) {
      currentAudioRef.current.pause();
      URL.revokeObjectURL(currentAudioRef.current.src);
      currentAudioRef.current = null;
    }

    // âœ… FERMA ANCHE SPEECH SYNTHESIS NATIVO
    speechCancel();

    // Cancella timeout
    if (processingTimeoutRef.current) {
      clearTimeout(processingTimeoutRef.current);
      processingTimeoutRef.current = null;
    }
  }, [speechCancel]);

  // Funzione per rispondere all'utente
  const respondWithVoice = useCallback(async (responseText: string) => {
    if (processingTimeoutRef.current) {
      clearTimeout(processingTimeoutRef.current);
      processingTimeoutRef.current = null;
    }
    
    const cleanText = responseText
      .replace(/[\u{1F000}-\u{1F9FF}]|[\u{2600}-\u{26FF}]|[\u{2700}-\u{27BF}]/gu, '')
      .replace(/[\u{FE00}-\u{FE0F}]/gu, '')
      .trim();
    
    setIsProcessing(false);
    await speakText(cleanText);
  }, [speakText]);

  // âœ… CONTROLLA SE QUALSIASI TIPO DI SINTESI Ãˆ ATTIVA
  const isCurrentlySpeaking = isSpeaking || speechSpeaking;

  return {
    // Stati
    isVoiceChatActive,
    isListening,
    isSpeaking: isCurrentlySpeaking, // âœ… Include anche Speech Synthesis nativo
    isProcessing,
    isVoiceChatActiveRef,
    // Azioni
    startVoiceChat,
    stopVoiceChat,
    respondWithVoice,
    speakText,
    // âœ… Nuove info utili
    speechSupported,
    hasCloudTTS: true,
    hasNativeTTS: speechSupported
  };
};